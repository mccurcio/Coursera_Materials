#Getting & cleaning data / Coursera Wk 1

## Checking for data in dir

if (!file.exists("data")) {
    dir.create("data")
}

## Getting data 
### note url, destination dir, methods(?)

file_url <- "http"
download.file(file_url, 
              destfile = "path.csv"
              method = "curl") 
list.files(path)

date_downloaded <- date() # give time info

## reading flat files

read.table()
read.csv()
na.strings = na

## Reading Excel files

download.file(file_url, 
              destfile = "path.xlsx"
              method = "curl") 
*or*

read.xlsx()
read.xlsx2() # faster

write.xlsx()
'XlConnect' package

## Reading XML
### 2 components: markup and content

start tags: <phrase>
stop tags: </phrase>
empthy tags: <phrase />

### Converet XML to DF
install.packages("xmlconvert", dependencies = TRUE)


### labels and attirbutes
<img src="" />
<text> text </text>

### XML Example

library(XML)
file_url <- "http"
doc_data <- xmlTreeParse(file_url,
                         useinternal = TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode) 

#### access first element

rootNode[[1]]

*or*

#### first of first sub-element

rootNode[[1]][[1]]

#### Programatically extract parts of the XML file

xmlSApply(rootNode, xmlValue)

#### Xpath language - GET TO KNOW?

see: https://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf

#### For specific parts of XML

xpathSApply(rootNode, "//name", xmlValue) # Gets name info


####Example from ESPN

file_url <- "espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc_data <- xmlTreeParse(file_url,
                         useinternal = TRUE)
scores <- xpathSApply(doc_data, "//li@class='score']", xmlValue)
scores


## Reading JSON
### structured language

See: wikipedia.org/wiki/JSON
Se: json.org
### r-bloggers jsonlite package

library(jsonlite)
jsonData <- fromJSON("api URL")
names(jsonData)

names(jsonData$owner) # second level

jsonData$owner$login # third level

### Example df to JSON
myjson_data <- toJSON(iris, pretty = TRUE)
cat(myjson_data)

### Example JSON to df
iris2 <- fromJSON(myjosn_data)
head(iris2)


## The data.table Package
### fast using C

Example for df
df = data.frame(x = rnorm(9),
                y = rep(c("a","b","c"), each = 3),
                z = rnorm(9))
head(df, 3)

### Example for data.table

library(data.table)
dt = data.table(x = rnorm(9),
                y = rep(c("a","b","c"), each = 3),
                z = rnorm(9))
head(dt, 3)

tables()

### subsetting row and cols
dt[2, ]
dt[dt$y = "a",]

dt[c(2,3)] #based on rows

### calcs dt
dt[, list(mean(x), sum(z))]
dt[, table(y)]

### adding new cols
dt[, w := z^2] # :=

### calcs with multiple operations
dt[, m:= {tmp <- (x+z); log2(tmp + 5)}]

### Plyr like operations
dt[, a := x>0] # returns true | false

### Special variables, .N
#### gives a 'count' of var x
set.seed(100)
dt <- data.table(x = sample(letters[1:3], 1e5, TRUE))
dt[, .N, by=x]

### data.table can have keys
#### subsetting by keys is fast
dt <- data.table(x = rep(c("a","b","c"), each = 100),
                 y = rnorm(300))
setkey(dt, x)
dt['a']

### Using keys to facilitate 'joins'
dt1 <- data.table(x = c('a', 'a', 'b', 'dt1'), y = 1:4)
dt2 <- data.table(x = c('a', 'b', 'dt2'), z = 5:7)

setkey(dt1,x)
setkey(dt2,x)
merge(dt1,dt2)

## Fast disk reads
big_df <- data.frame(x = rnorm(1e6), y = rnorm(1e6))
file <- tempfile()
write.table(big_df, 
            file=file, 
            row.names=FALSE, 
            col.names=TRUE,
            sep='\t',
            quote=FALSE)
system.time(fread(file)) # fread use for data.tables

> system.time(fread(file))
  user  system elapsed 
  0.107   0.000   0.020 


system.time(read.table(file, header=TRUE, sep='\t'))

>  user  system elapsed 
   2.781   0.040   2.820 

### data.table now have the command `melt`, `dcast`
### See: stackoverflow.com/questions/13618488/what-you-can-do-with-data-frame-that-you-cant-in-data-table

---

## XML note from S.O.

https://www.youtube.com/watch?v=1cM_ZNZ9hhE

###################
### reading xml in to R to DF

library("XML")

url <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"

download.file(url, "restaurants.xml")

doc <- xmlTreeParse("restaurants.xml", useInternalNodes=TRUE)
rootNode = xmlRoot(doc)
xmlName(rootNode)

names(rootNode)







xml_data_frame <- xmlToDataFrame("path.xml")

print(xml_data_frame)

library(XML)> # Fetch the XML content and extract the root nodeurl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"xml <- xmlTreeParse(url, useInternalNodes = TRUE)xmlroot <- xmlRoot(xml)> # Extract the element names from the first node representing the first restaurant> as.character(sapply(xmlroot[[1]][1], function(x) xmlSApply(x, xmlName)))> [1] "name"            "zipcode"         "neighborhood"    "councildistrict"  [5] "policedistrict"  "location_1"

















